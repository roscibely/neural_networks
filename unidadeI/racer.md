# DeepRacer - AWS

## ğŸš— AWS DeepRacer


<img src="https://hostingjournalist.com/wp-content/uploads/2020/01/AWS-DeepRacer-League-Preview.jpg" width="200" height="200" />

### ğŸ“š O que Ã© o AWS DeepRacer?

O AWS DeepRacer Ã© um carro de corrida autÃ´nomo em escala 1/18  treinado com redes neurais utilizando aprendizado de reforÃ§o (Reinforcement Learning - RL). O AWS DeepRacer foi projetado para fornecer uma maneira divertida e interessante de comeÃ§ar a usar o aprendizado de reforÃ§o (RL). 

O AWS DeepRacer Ã© um veÃ­culo de cÃ³digo aberto, com uma comunidade de desenvolvedores ativa e crescente, que permite que vocÃª experimente e compartilhe novas ideias e recursos.

### ğŸ“š O que Ã© o aprendizado de reforÃ§o?

O aprendizado de reforÃ§o Ã© uma tÃ©cnica de aprendizado de mÃ¡quina que ensina um agente a aprender o comportamento ideal em um ambiente executando aÃ§Ãµes e vendo os resultados. O agente aprende a alcanÃ§ar uma meta em um ambiente incerto, onde ele nÃ£o Ã© informado qual aÃ§Ã£o executar, mas Ã© recompensado ou punido por suas aÃ§Ãµes. O agente deve, portanto, determinar por si mesmo qual aÃ§Ã£o executar para maximizar a recompensa ao longo do tempo.

### ğŸ“š Como funciona o AWS DeepRacer?

VocÃª primeiro criar sua conta clicando em [AWS DeepRacer](https://student.deepracer.com/home) e depois de criar sua conta, vocÃª pode acessar o console do AWS DeepRacer e comeÃ§ar a treinar seu modelo de aprendizado de reforÃ§o.

### Como treinar seu modelo de aprendizado de reforÃ§o?

Para treinar seu modelo de aprendizado de reforÃ§o, vocÃª precisa criar um modelo de aprendizado de reforÃ§o, criar uma pista de corrida, treinar seu modelo e avaliar seu modelo.

#### Criando um modelo de aprendizado de reforÃ§o

Para criar um modelo de aprendizado de reforÃ§o, vocÃª precisa acessar o console do AWS DeepRacer e clicar em **Create model**.

VocÃª precisarÃ¡ definir uma funÃ§Ã£o de recompensa, um algoritmo de aprendizado de reforÃ§o e um ambiente de simulaÃ§Ã£o.


# Exemplo de funÃ§Ã£o de recompensa

```python
def reward_function(params):
    '''
    Example of rewarding the agent to follow center line
    '''

    # Read input parameters
    track_width = params['track_width'] 
    distance_from_center = params['distance_from_center']

    # Calculate 3 markers that are at varying distances away from the center line
    marker_1 = 0.1 * track_width
    marker_2 = 0.25 * track_width
    marker_3 = 0.5 * track_width

    # Give higher reward if the car is closer to center line and vice versa
    if distance_from_center <= marker_1:
        reward = 1.0 # maximum reward
    elif distance_from_center <= marker_2:
        reward = 0.5 # intermediate reward
    elif distance_from_center <= marker_3:
        reward = 0.1 # intermediate reward
    else:
        reward = 1e-3  # likely crashed/ close to off track. We penalize.

    return float(reward)
```

## Video de simulaÃ§Ã£o 

![](https://github.com/roscibely/neural_networks/blob/develop/unidadeI/animation.gif)
